{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "472dbd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/medaracaityte/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas de manipulação de dados\n",
    "import pandas as pd\n",
    "\n",
    "# Processamento de texto e NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet as wn, sentiwordnet as swn\n",
    "from googletrans import Translator\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Métricas e avaliação\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, f1_score,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71291295",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c8911858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "test_df = pd.read_csv('amazon_reviews_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adeb0d",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b69b690",
   "metadata": {},
   "source": [
    "#### Funções de Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "be9cd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção de duplicados\n",
    "def remove_duplicate_words(text):\n",
    "    \"\"\"\n",
    "    Remove palavras duplicadas de um texto, preservando a ordem da primeira ocorrência.\n",
    "    Exemplo: 'bom bom dia dia' -> 'bom dia'\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            unique_words.append(word)\n",
    "    return ' '.join(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4e5a5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traduzir as reviews não-inglesas para inglês\n",
    "translator = Translator()\n",
    "# Função para traduzir texto para inglês\n",
    "def translate_to_english(text, src_lang):\n",
    "    if src_lang == 'en' or src_lang == 'error':\n",
    "        return text  # Não traduzir se já for inglês ou erro na deteção\n",
    "    try:\n",
    "        translation = translator.translate(text, src=src_lang, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao traduzir: {e}\")\n",
    "        return text  # Se falhar, devolve o texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a91ff03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função de Tokenização\n",
    "def word_based_tokenization(text):\n",
    "    # Usar os espaços para dividir em tokens\n",
    "    tokens_space = text.split()\n",
    "\n",
    "    # Separar sinais de pontuação (ex.: \".\", \",\") usando regex\n",
    "    text_with_punct = re.sub(r'([.,!?\"])', r' \\1 ', text)  # Adiciona espaço ao redor da pontuação\n",
    "    text_with_punct = re.sub(r'\\s+', ' ', text_with_punct)  # Remove múltiplos espaços\n",
    "\n",
    "    # Tokenizar novamente após manipulação de pontuação e converter para minúsculas\n",
    "    tokens_lower = [token.lower() for token in text_with_punct.split()]\n",
    "\n",
    "    return ' '.join(tokens_lower)  # Retorna os tokens como uma string unificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f2a57c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagar quebras de linha e fazer casefolding\n",
    "def remove_newlines_and_lower(text):\n",
    "    text = re.sub(r'[\\n\\r]', '', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f3159ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover tags HTML específicas\n",
    "def remove_specific_html_tags(text):\n",
    "    # Substituir <br /> por quebra de linha\n",
    "    text = text.replace('<br />', '\\n')\n",
    "    # Remover <a> e </a>\n",
    "    text = re.sub(r'<a[^>]*>', '', text)  # Remove <a href=\"\">\n",
    "    text = text.replace('</a>', '')\n",
    "    # Remover <span class=\"tiny\"> e </span>\n",
    "    text = re.sub(r'<span[^>]*>', '', text)  # Remove <span class=\"tiny\">\n",
    "    text = text.replace('</span>', '')\n",
    "    # Remover <p> e </p>\n",
    "    text = text.replace('<p>', '').replace('</p>', '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8feeefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar e remover caracteres estranhos\n",
    "padrao = re.compile(r\"[^a-zA-Z0-9\\s.,;:!?\\\"'()\\[\\]{}@#%&/\\-_=+<>€$£]\")\n",
    "\n",
    "def remove_caracteres_estranhos(text):\n",
    "    return padrao.sub('', str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "14eb5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover espaços duplos\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "566fa8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituição dos emojis por texto\n",
    "def replace_emojis(text):\n",
    "    \"\"\"\n",
    "    Substitui emojis por representações textuais.\n",
    "    \"\"\"\n",
    "    emoji_dict = {\n",
    "        ':)': 'good',\n",
    "        ':-)': 'good',\n",
    "        ':-D': 'good',\n",
    "        ':D': 'good',\n",
    "        ':(': 'bad',\n",
    "        ':-(': 'bad',\n",
    "        ':O': 'surprised',\n",
    "    }\n",
    "\n",
    "    # Substitui cada emoji pelo texto correspondente\n",
    "    for emoji, emotion in emoji_dict.items():\n",
    "        text = text.replace(emoji, emotion)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8a82d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção das datas\n",
    "def remove_dates(text):\n",
    "    # Remove formatos comuns de datas: 14/04/2025, 04-14-2025, 2023-04-14, etc.\n",
    "    text = re.sub(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b', '', text)  # Datas tipo 14/04/2025 ou 2023-04-14\n",
    "    text = re.sub(r'\\b\\d{4}\\b', '', text)  # Anos isolados (ex.: 2023)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e4c1d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção dos números\n",
    "def remove_numbers(text):\n",
    "    # Remove números inteiros e decimais isolados\n",
    "    text = re.sub(r'\\b\\d+\\.\\d+\\b', '', text)  # Números decimais (ex.: 45.67)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)       # Números inteiros (ex.: 123)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a0d839b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função de negação\n",
    "def preprocess_negation(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    result = []\n",
    "    negation_active = False\n",
    "    for word in words:\n",
    "        if word in [\"not\", \"no\", \"never\", \"don't\", \"didn't\", \"doesn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"isn't\", \"ain't\", \"hasn't\", \"haven't\", \"hadn't\", \"wasn't\", \"weren't\", \"neither\", \"nor\"]:\n",
    "            negation_active = True\n",
    "            result.append(word)\n",
    "            continue\n",
    "        if word in [\".\", \",\", \"!\", \"?\", \":\", \";\", \"but\",\"however\",\"although\",\"even though\",\"despite\",\"in spite of\"]:\n",
    "            negation_active = False\n",
    "            result.append(word)\n",
    "            continue\n",
    "        if negation_active:\n",
    "            result.append(\"NOT_\" + word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ad138",
   "metadata": {},
   "source": [
    "#### Funções NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "18693508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para NRC Lexicon (com negação e tokenização opcionais)\n",
    "emolex = pd.read_csv('NCR-lexicon.csv', delimiter=\";\", encoding=\"utf-8\")[['English', 'Positive', 'Negative']]\n",
    "\n",
    "def lexicon_sentiment(text, use_negation=False, use_preprocessing=False):\n",
    "    if use_negation:\n",
    "        text = preprocess_negation(text)\n",
    "    if use_preprocessing:\n",
    "        #text = remove_duplicate_words(text)\n",
    "        #text = translate_to_english(text, src_lang='en')\n",
    "        text = word_based_tokenization(text)\n",
    "        text = remove_newlines_and_lower(text)\n",
    "        text = remove_specific_html_tags(text)\n",
    "        text = remove_multiple_spaces(text)\n",
    "        text = remove_dates(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_caracteres_estranhos(text)\n",
    "        text = replace_emojis(text)\n",
    "    words = text.split()\n",
    "    pos_count = neg_count = 0\n",
    "    for word in words:\n",
    "        if word.startswith(\"NOT_\"):\n",
    "            original_word = word[4:]\n",
    "            entry = emolex[emolex['English'] == original_word]\n",
    "            if not entry.empty:\n",
    "                pos_count += entry['Negative'].values[0]\n",
    "                neg_count += entry['Positive'].values[0]\n",
    "        else:\n",
    "            entry = emolex[emolex['English'] == word]\n",
    "            if not entry.empty:\n",
    "                pos_count += entry['Positive'].values[0]\n",
    "                neg_count += entry['Negative'].values[0]\n",
    "    return 'positive' if pos_count > neg_count else 'negative'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb95094",
   "metadata": {},
   "source": [
    "#### Funções SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0582698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares para SentiWordNet\n",
    "def convert_tag(tag):\n",
    "    tag_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except:\n",
    "        return wn.NOUN\n",
    "\n",
    "def sentiwordnet_analysis(text, use_negation=False, use_preprocessing=False):\n",
    "    if use_negation:\n",
    "        text = preprocess_negation(text)\n",
    "    if use_preprocessing:\n",
    "        #text = remove_duplicate_words(text)\n",
    "        #text = translate_to_english(text, src_lang='en')\n",
    "        #text = word_based_tokenization(text)\n",
    "        text = remove_newlines_and_lower(text)\n",
    "        text = remove_specific_html_tags(text)\n",
    "        text = remove_multiple_spaces(text)\n",
    "        text = remove_dates(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_caracteres_estranhos(text)\n",
    "        text = replace_emojis(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for word, tag in tagged:\n",
    "        wn_tag = convert_tag(tag)\n",
    "        if not wn_tag:\n",
    "            continue\n",
    "        if word.startswith(\"NOT_\"):\n",
    "            original_word = word[4:]\n",
    "            synsets = list(swn.senti_synsets(original_word, wn_tag))\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                pos_score += synset.neg_score()\n",
    "                neg_score += synset.pos_score()\n",
    "        else:\n",
    "            synsets = list(swn.senti_synsets(word, wn_tag))\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                pos_score += synset.pos_score()\n",
    "                neg_score += synset.neg_score()\n",
    "    return 'positive' if pos_score > neg_score else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e3080",
   "metadata": {},
   "source": [
    "#### Funções VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2c78f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o VADER\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Função para VADER\n",
    "def vader_sentiment(text, use_negation=False, use_preprocessing=False):\n",
    "    if use_negation:\n",
    "        #text = remove_duplicate_words(text)\n",
    "        #text = translate_to_english(text, src_lang='en')\n",
    "        text = word_based_tokenization(text)\n",
    "        text = remove_newlines_and_lower(text)\n",
    "        text = remove_specific_html_tags(text)\n",
    "        text = remove_multiple_spaces(text)\n",
    "        text = remove_dates(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_caracteres_estranhos(text)\n",
    "        text = replace_emojis(text)\n",
    "    if use_preprocessing:\n",
    "        text = word_based_tokenization(text)\n",
    "    scores = vader_analyzer.polarity_scores(text)\n",
    "    return 'positive' if scores['compound'] > 0 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fb4f2",
   "metadata": {},
   "source": [
    "### NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "10ad5164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRC - Sem Negação e Pré-processamento:\n",
      "Accuracy: 0.6429458005792305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.45      0.44       741\n",
      "    positive       0.75      0.73      0.74      1676\n",
      "\n",
      "    accuracy                           0.64      2417\n",
      "   macro avg       0.59      0.59      0.59      2417\n",
      "weighted avg       0.65      0.64      0.65      2417\n",
      "\n",
      "\n",
      "NRC - Sem Negação Com Pré-processamento:\n",
      "Accuracy: 0.6677699627637568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.40      0.43       741\n",
      "    positive       0.75      0.78      0.77      1676\n",
      "\n",
      "    accuracy                           0.67      2417\n",
      "   macro avg       0.60      0.59      0.60      2417\n",
      "weighted avg       0.66      0.67      0.66      2417\n",
      "\n",
      "\n",
      "NRC - Com Negação Sem Pré-processamento:\n",
      "Accuracy: 0.6839056681836988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.50      0.49       741\n",
      "    positive       0.78      0.76      0.77      1676\n",
      "\n",
      "    accuracy                           0.68      2417\n",
      "   macro avg       0.63      0.63      0.63      2417\n",
      "weighted avg       0.69      0.68      0.69      2417\n",
      "\n",
      "\n",
      "NRC - Com Negação e Pré-processamento:\n",
      "Accuracy: 0.6797683078196111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.45      0.46       741\n",
      "    positive       0.76      0.78      0.77      1676\n",
      "\n",
      "    accuracy                           0.68      2417\n",
      "   macro avg       0.62      0.62      0.62      2417\n",
      "weighted avg       0.68      0.68      0.68      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicação das funções feitas no código inicial\n",
    "# NRC Lexicon\n",
    "test_df['NRC_no_negation_or_preprocessing'] = test_df['review'].apply(lexicon_sentiment, use_negation=False, use_preprocessing=False)\n",
    "test_df['NRC_no_negation_with_preprocessing'] = test_df['review'].apply(lexicon_sentiment, use_negation=False, use_preprocessing=True)\n",
    "test_df['NRC_with_negation_no_preprocessing'] = test_df['review'].apply(lexicon_sentiment, use_negation=True, use_preprocessing=False)\n",
    "test_df['NRC_with_negation_and_preprocessing'] = test_df['review'].apply(lexicon_sentiment, use_negation=True, use_preprocessing=True)\n",
    "\n",
    "print(\"\\nNRC - Sem Negação e Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['NRC_no_negation_or_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['NRC_no_negation_or_preprocessing']))\n",
    "print(\"\\nNRC - Sem Negação Com Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['NRC_no_negation_with_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['NRC_no_negation_with_preprocessing']))\n",
    "print(\"\\nNRC - Com Negação Sem Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['NRC_with_negation_no_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['NRC_with_negation_no_preprocessing']))\n",
    "print(\"\\nNRC - Com Negação e Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['NRC_with_negation_and_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['NRC_with_negation_and_preprocessing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af57bd",
   "metadata": {},
   "source": [
    "### SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "48c71e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SentiWordNet - Sem Negação e Pré-processamento:\n",
      "Accuracy: 0.7190732312784444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.46      0.50       741\n",
      "    positive       0.78      0.84      0.80      1676\n",
      "\n",
      "    accuracy                           0.72      2417\n",
      "   macro avg       0.66      0.65      0.65      2417\n",
      "weighted avg       0.71      0.72      0.71      2417\n",
      "\n",
      "\n",
      "SentiWordNet - Sem Negação Com Pré-processamento:\n",
      "Accuracy: 0.7223831195697146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.50      0.53       741\n",
      "    positive       0.79      0.82      0.80      1676\n",
      "\n",
      "    accuracy                           0.72      2417\n",
      "   macro avg       0.67      0.66      0.67      2417\n",
      "weighted avg       0.72      0.72      0.72      2417\n",
      "\n",
      "\n",
      "SentiWordNet - Com Negação Sem Pré-processamento:\n",
      "Accuracy: 0.7261067438973935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.55      0.55       741\n",
      "    positive       0.80      0.80      0.80      1676\n",
      "\n",
      "    accuracy                           0.73      2417\n",
      "   macro avg       0.68      0.68      0.68      2417\n",
      "weighted avg       0.73      0.73      0.73      2417\n",
      "\n",
      "\n",
      "SentiWordNet - Com Negação e Pré-processamento:\n",
      "Accuracy: 0.7306578402978899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.54      0.55       741\n",
      "    positive       0.80      0.81      0.81      1676\n",
      "\n",
      "    accuracy                           0.73      2417\n",
      "   macro avg       0.68      0.68      0.68      2417\n",
      "weighted avg       0.73      0.73      0.73      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicação das funções feitas no código inicial\n",
    "# SentiWordNet Lexicon\n",
    "test_df['SWN_no_negation_or_preprocessing'] = test_df['review'].apply(sentiwordnet_analysis, use_negation=False, use_preprocessing=False)\n",
    "test_df['SWN_no_negation_with_preprocessing'] = test_df['review'].apply(sentiwordnet_analysis, use_negation=False, use_preprocessing=True)\n",
    "test_df['SWN_with_negation_no_preprocessing'] = test_df['review'].apply(sentiwordnet_analysis, use_negation=True, use_preprocessing=False)\n",
    "test_df['SWN_with_negation_and_preprocessing'] = test_df['review'].apply(sentiwordnet_analysis, use_negation=True, use_preprocessing=True)\n",
    "\n",
    "print(\"\\nSentiWordNet - Sem Negação e Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['SWN_no_negation_or_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['SWN_no_negation_or_preprocessing']))\n",
    "print(\"\\nSentiWordNet - Sem Negação Com Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['SWN_no_negation_with_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['SWN_no_negation_with_preprocessing']))\n",
    "print(\"\\nSentiWordNet - Com Negação Sem Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['SWN_with_negation_no_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['SWN_with_negation_no_preprocessing']))\n",
    "print(\"\\nSentiWordNet - Com Negação e Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['SWN_with_negation_and_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['SWN_with_negation_and_preprocessing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c64c11",
   "metadata": {},
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a133dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VADER - Sem Negação e Pré-processamento:\n",
      "Accuracy: 0.7894083574679355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.44      0.56       741\n",
      "    positive       0.79      0.94      0.86      1676\n",
      "\n",
      "    accuracy                           0.79      2417\n",
      "   macro avg       0.78      0.69      0.71      2417\n",
      "weighted avg       0.79      0.79      0.77      2417\n",
      "\n",
      "\n",
      "VADER - Sem Negação Com Pré-processamento:\n",
      "Accuracy: 0.7931319817956144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.44      0.57       741\n",
      "    positive       0.79      0.95      0.86      1676\n",
      "\n",
      "    accuracy                           0.79      2417\n",
      "   macro avg       0.79      0.69      0.72      2417\n",
      "weighted avg       0.79      0.79      0.77      2417\n",
      "\n",
      "\n",
      "VADER - Com Negação Sem Pré-processamento:\n",
      "Accuracy: 0.7935457178320232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.44      0.57       741\n",
      "    positive       0.79      0.95      0.86      1676\n",
      "\n",
      "    accuracy                           0.79      2417\n",
      "   macro avg       0.79      0.70      0.72      2417\n",
      "weighted avg       0.79      0.79      0.77      2417\n",
      "\n",
      "\n",
      "VADER - Com Negação e Pré-processamento:\n",
      "Accuracy: 0.7935457178320232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.44      0.57       741\n",
      "    positive       0.79      0.95      0.86      1676\n",
      "\n",
      "    accuracy                           0.79      2417\n",
      "   macro avg       0.79      0.70      0.72      2417\n",
      "weighted avg       0.79      0.79      0.77      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicação das funções feitas no código inicial\n",
    "# VADER\n",
    "test_df['VADER_no_negation_or_preprocessing'] = test_df['review'].apply(vader_sentiment, use_negation=False, use_preprocessing=False)\n",
    "test_df['VADER_no_negation_with_preprocessing'] = test_df['review'].apply(vader_sentiment, use_negation=False, use_preprocessing=True)\n",
    "test_df['VADER_with_negation_no_preprocessing'] = test_df['review'].apply(vader_sentiment, use_negation=True, use_preprocessing=False)\n",
    "test_df['VADER_with_negation_and_preprocessing'] = test_df['review'].apply(vader_sentiment, use_negation=True, use_preprocessing=True)\n",
    "print(\"\\nVADER - Sem Negação e Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['VADER_no_negation_or_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['VADER_no_negation_or_preprocessing']))\n",
    "print(\"\\nVADER - Sem Negação Com Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['VADER_no_negation_with_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['VADER_no_negation_with_preprocessing']))\n",
    "print(\"\\nVADER - Com Negação Sem Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['VADER_with_negation_no_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['VADER_with_negation_no_preprocessing']))\n",
    "print(\"\\nVADER - Com Negação e Pré-processamento:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_df['sentiment'], test_df['VADER_with_negation_and_preprocessing']))\n",
    "print(classification_report(test_df['sentiment'], test_df['VADER_with_negation_and_preprocessing']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
